# 支持向量机练习
本练习参考了bilibili视频：https://www.bilibili.com/video/BV1TZ4y187qX?p=119	部分代码有所变动

## 基本支持向量机

查看不使用支持向量机和使用支持向量机的效果：

```shell
python basic_svm.py
```

## 加入软间隔

对比使用不同惩罚超参数C的分类效果：

```shell
python soft_margin.py
```

需要注意：

如果运行的服务器中没有GUI引擎，则不能使用`plt.show()`来展示图片。

## 非线性分类器

查看非线性分类的效果：

```shell
python poly.py
```

本例中使用了`PolynomialFeatures`将数据升维至`degree=3`。



### 一些numpy函数的解释

- `np.meshgrid(x0s,x1s)`

    ​	返回`x_mesh`和`y_mesh`，两者的形状由输入的`x0s`和`x1s`决定，例子：

    ```python
    x0s = np.array([1,2,3,...,100])
    x1s = np.array([1,2,3,...,100])
    x_mesh = np.array(
        [1,2,3,...,100],
        [1,2,3,...,100],
        .
        .
        .
        [1,2,3,...,100]
    )
    y_mesh = np.array(
        [  1,  1,...,  1],
        [  2,  2,...,  2],
        [100,100,...,100]
    )
    ```

- `np.ravel()`

    将一个多维数组拉成一个一维数组

    ```
    以上面的x_mesh为例
    x_mesh.ravel() = 
    [1,2,3,...100,1,2,3,...,100,....,100]
    ```

- `np.c_`

    以深入的第一层为对象，进行行拼接

    ```python
    x_1 = np.array([1, 2, 3, 4, 5, 6]).reshape(2, 3)
    x_2 = np.array([3, 2, 1, 8, 9, 6]).reshape(2, 3)
    x_new = np.c_[x_1,x_2]
    
    x_3 = np.array([1, 2, 3, 4, 5, 6])
    x_4 = np.array([3, 2, 1, 8, 9, 6])
    x_new1 = np.c_[x_3,x_4]
    
    结果：
    x_1 = 
     [[1 2 3]
     [4 5 6]]
    x_2 = 
     [[3 2 1]
     [8 9 6]]
    x_new = 
     [[1 2 3 3 2 1]
     [4 5 6 8 9 6]]
    x_new1 = 
     [[1 3]
     [2 2]
     [3 1]
     [4 8]
     [5 9]
     [6 6]]
    ```

    **什么为之深入的第一层？**

    比如说`x_1`和`x_2`进行拼接，深入第一层发现其中是一个一维数组，因此拼接对象就是这个一维数组，并且拼接的方式是同一行的进行拼接。

    而对于`x_3`和`x_4`，输入第一层发现是一个元素，因此以一个元素为一行，两个元素进行拼接，组成新的一行。

    **为什么需要进行一系列的结构转换?**

    确定目标：通过等高线来确定分割线

    1. 绘制等高线的关键是建立网格，通过`meshgrid`函数建立网格，获得`x0`和`x1`两个网格向量
    2. 因为需要通过支持向量机进行预测，所以考虑支持向量机的输入应该是对应每一个网格点，相应输出一个预测值，于是将`x0`和`x1`整理成10000×2的矩阵，即网格内的10000个坐标
    3. `plt.contourf`函数绘制等高线，给出x轴和y轴的坐标向量，再给出z轴的坐标向量，因此x,y,z向量的形状应当是一样的，这样才能做到一一对应，所以调用`reshape()`函数即可。



### 效果展示与分析

![poly.png](https://github.com/Server-Not-Found/BasicML/blob/master/SVM/figures/poly.png?raw=true)



左图为C=10，右图为C=1，可以观察到作图的惩罚力度比较大，但是仍然容许一小部分的黄点越过边界，而C=1时有比较多的黄点越过了边界。


## 核函数

观察`poly`和`rbf`核函数的分类效果：

```shell
python kernel.py
```

核函数的原理在此不再赘述，可阅读PDF文档

### 高斯核函数的相关参数

$\gamma$:反映了高斯分布的泛化程度，$\gamma$越大，高斯分布的宽度越窄，因此只有在两个样本点的欧式距离非常小时才能认为两个样本点具有相似特征，如果$\gamma$很大，则分类器缺乏泛化能力。以高斯核函数为例：

![rbf_kernel.png](https://github.com/Server-Not-Found/BasicML/blob/master/SVM/figures/rbf_kernel.png?raw=true)



横向比较惩罚超参数C带来的效果，纵向比较$\gamma$带来的效果，可以发现再在$\gamma$很大的时候，模型已经严重过拟合了，而在惩罚参数和$\gamma$都很小时，因为对松弛变量惩罚力度不够的同时，高斯分布的泛化程度也较高，因此模型是欠拟合的。